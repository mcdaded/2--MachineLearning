{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Titanic Competition\n",
    "\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#Split values based on the two CSVs\n",
    "training = pd.read_csv(\"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\")\n",
    "testing = pd.read_csv(\"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\")\n",
    "#create boolean column to break training and testing data. but we will keep them combined for imputation\n",
    "training['training_data'] = True\n",
    "testing['training_data'] = False\n",
    "data = pd.concat([training, testing], axis=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorize_values(value, column):\n",
    "    \"\"\"\n",
    "    Apply categories based on percentiles of data\n",
    "    \"\"\"\n",
    "    if value <= data[[column]].describe().ix['25%'].values: #7.91\n",
    "        category = 0\n",
    "    elif value >= data[[column]].describe().ix['75%'].values: #31\n",
    "        category = 2\n",
    "    else:\n",
    "        category=1\n",
    "    return category\n",
    "\n",
    "def survival_ratios(data_series, column_name, df):\n",
    "    \"\"\"\n",
    "    Return pivot ratios of the data and survival rates\n",
    "    \"\"\"\n",
    "    data_values = pd.concat([df[['Survived','PassengerId']], \n",
    "                           pd.DataFrame(data=data_series, columns=column_name)], axis=1 )\n",
    "    pvt_data = pd.pivot_table(data=data_values, index=['Survived'],\n",
    "                               columns=column_name, values =['PassengerId'], aggfunc='count')\n",
    "    return pvt_data.apply(lambda x: x / (x.max() + x.min()))\n",
    "\n",
    "def title_extract(name):\n",
    "    title = name.split(',')[1].split('.')[0].strip()\n",
    "    if title in title_map: title = title_map[title]\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "survival_ratios(data[['Age']].values, ['Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#-------------------------------------------------------------------\n",
    "# pipelines and scalars\n",
    "#-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA, RandomizedPCA\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.grid_search.RandomizedSearchCV\n",
    "#from sklearn.grid_search.ParameterGrid\n",
    "#from sklearn.grid_search.ParameterSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data['child'] = (data.Age.dropna() <= 18)*1\n",
    "\n",
    "data['expensive_coach_woman'] = ((data.Sex == 'female') & (data.Pclass==3) & (data.Fare >= 20)) * 1\n",
    "# based on looking at the values there are a few that appear very infrequently, we will map them to more useful values\n",
    "title_map = {'Capt': 'Sir', 'Don':'Sir', 'Major':'Sir', 'Sir':'Sir', 'Col':'Sir', 'Mlle':'Sir', 'Jonkheer':'Sir',\n",
    "             'Mme': 'Lady', 'Lady':'Lady', 'the Countess':'Lady', 'Ms': 'Miss'}\n",
    "\n",
    "data['Title'] = data.Name.apply(lambda x: title_extract(x))\n",
    "data['Surname'] = data.Name.apply(lambda x: x.split(',')[0])\n",
    "data['Family_size'] = data.SibSp + data.Parch + 1\n",
    "data['FamilyID'] = (data.Family_size.astype(str) + data.Surname) * ((data.Family_size >= 2) * 1)\n",
    "data['FamilyID'] = data['FamilyID'].apply(lambda x: 'small' if x == '' else x)\n",
    "data['TicketCategory'] = data['Fare'].apply(lambda x: categorize_values(x, 'Fare'))\n",
    "data['AgeCategory'] = data['Fare'].apply(lambda x: categorize_values(x, 'Age'))\n",
    "data['FamilyCategory'] = data['Family_size'].apply(lambda x: categorize_values(x, 'Family_size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features[features.training_data].drop(['Survived', 'training_data'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'Fare', 'Age',\n",
    "features = pd.concat([data[['expensive_coach_woman', 'Pclass', 'TicketCategory', 'FamilyCategory']],\n",
    "                      pd.get_dummies(data['Sex'], prefix='Sex'),\n",
    "                      #looks like these are the only useful\n",
    "                      pd.get_dummies(data[data['Title'].isin(['Mr','Miss','Mrs'])]['Title']), \n",
    "                      pd.get_dummies(data['Embarked'], prefix='Embarked'),\n",
    "                     data[['training_data','Survived', 'PassengerId']], ], axis=1)\n",
    "# because the data is only male or femail we will drop the 'Sex_male' column as it is duplicated\n",
    "features = features.drop('Sex_male', 1)\n",
    "\n",
    "# We will try to loop through the different classes and sex to apply the median age\n",
    "#now we loop through the sex and cabin class values and apply the median age valuesto the groups\n",
    "for sex in xrange(0,2):\n",
    "    for cabin in xrange(0,4):\n",
    "        median_age = data[(features.Pclass == cabin) & (features.Sex_female == sex)].dropna().median()\n",
    "        features[(features.Pclass == cabin) & (features.Sex_female == sex)\n",
    "                    ] = features[(features.Pclass == cabin) & (features.Sex_female == sex) ].fillna(median_age)\n",
    "\n",
    "target = features[features.training_data].Survived\n",
    "features = features.fillna(0)\n",
    "training_features = features[features.training_data].drop(['Survived','training_data','PassengerId'],  1)\n",
    "kaggle_data = features[~features.training_data].drop(['Survived','training_data'], 1)\n",
    "\n",
    "#features = features.fillna(-1)\n",
    "\n",
    "# Because of the following bug we cannot use NaN as the missing\n",
    "# value marker, use a negative value as marker instead:\n",
    "# https://github.com/scikit-learn/scikit-learn/issues/3044\n",
    "training_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, RandomizedPCA\n",
    "\n",
    "pca = RandomizedPCA()\n",
    "#print pca.explained_variance_ratio_\n",
    "pca.fit(pd.concat([pd.DataFrame(target), training_features], axis=1))\n",
    "\n",
    "pca_big = PCA().fit(training_features[['Sex_female','TicketCategory']], target)\n",
    "plt.title(\"Explained Variance\")\n",
    "plt.ylabel(\"Percentage of explained variance\")\n",
    "plt.xlabel(\"PCA Components\")\n",
    "plt.plot(pca.explained_variance_ratio_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Minimum percentage of variance we want to be described by the resulting transformed components\n",
    "variance_pct = .99\n",
    " \n",
    "# Create PCA object\n",
    "pca = PCA(n_components=variance_pct)\n",
    " \n",
    "# Transform the initial features\n",
    "X_transformed = pca.fit_transform(training_features.values, target)\n",
    " \n",
    "# Create a data frame from the PCA'd data\n",
    "pcaDataFrame = pd.DataFrame(X_transformed)\n",
    " \n",
    "print pcaDataFrame.shape[1], \" components describe \", str(variance_pct)[1:], \"% of the variance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imputer = Imputer(strategy='median', missing_values=-1)\n",
    "scalar = StandardScaler()\n",
    "pca = PCA()\n",
    "classifiers = [RandomForestClassifier(), GradientBoostingClassifier(), \n",
    "               ExtraTreesClassifier(), LogisticRegression(), DecisionTreeClassifier(),\n",
    "               KNeighborsClassifier(), SVC(), LinearSVC(), GaussianNB()]\n",
    "\n",
    "print '\\n'\n",
    "for classifier in classifiers:\n",
    "    pipeline = Pipeline([\n",
    "        #('scl', scalar),\n",
    "        ('imp', imputer),\n",
    "        ('clf', classifier),\n",
    "    ])\n",
    "    #scores = cross_val_score(pipeline, features.values, target, cv=5, scoring='accuracy')\n",
    "    scores = cross_val_score(pipeline, training_features, target, cv=10, scoring='accuracy')\n",
    "    print str(classifier)\n",
    "    print('min -', scores.min(), 'mean -', scores.mean(), 'max -', scores.max())\n",
    "    print '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imputer = Imputer(strategy='median', missing_values=-1)\n",
    "scalar = StandardScaler()\n",
    "pca = PCA()\n",
    "\n",
    "classifier = GradientBoostingClassifier(n_estimators=100, subsample=.8)\n",
    "\n",
    "params = {\n",
    "    #'pca__n_components': np.arange(2,features.shape[1]+1).tolist() ,\n",
    "    'clf__learning_rate': [0.1, 0.15, 0.2],\n",
    "    'clf__max_features': [0.76, 0.77, 0.78],\n",
    "    'clf__max_depth': [3.6, 3.63],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('scl', scalar),\n",
    "    ('imp', imputer),\n",
    "    # ('pca', pca),\n",
    "    ('clf', classifier),\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, params, cv=10, scoring='accuracy')\n",
    "grid_search.fit(training_features.values, target)\n",
    "print grid_search.best_score_\n",
    "print grid_search.best_params_\n",
    "# with pca\n",
    "# 0.866883208974\n",
    "# {'clf__max_features': 0.77, 'clf__max_depth': 3.63, 'clf__learning_rate': 0.1, 'pca__n_components': 3}\n",
    "# without pca without scalar\n",
    "# 0.87819667245\n",
    "# {'clf__max_features': 0.77, 'clf__max_depth': 3.63, 'clf__learning_rate': 0.1}\n",
    "# without pca with scalar\n",
    "# 0.878755362749\n",
    "# {'clf__max_features': 0.76, 'clf__max_depth': 3.63, 'clf__learning_rate': 0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imputer = Imputer(strategy='median', missing_values=-1)\n",
    "scalar = StandardScaler()\n",
    "pca = PCA()\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "params = {\n",
    "   # 'pca__n_components': np.arange(2,features.shape[1]+1).tolist() ,\n",
    "    'clf__criterion':['gini','entropy'],\n",
    "    'clf__max_features': [0.75, 0.8, 0.9],\n",
    "    'clf__max_depth': [3.75, 3.9, 4, 4.1],\n",
    "    'clf__min_samples_split': [2, 10, 50],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('scl', scalar),\n",
    "    ('imp', imputer),\n",
    "    #('pca', pca),\n",
    "    ('clf', classifier),\n",
    "])\n",
    "\n",
    "grid_search2 = GridSearchCV(pipeline, params, cv=10, scoring='accuracy')\n",
    "grid_search2.fit(training_features.values, target)\n",
    "print grid_search2.best_score_\n",
    "print grid_search2.best_params_\n",
    "# with pca\n",
    "# 0.86815556909\n",
    "# {'pca__n_components': 15, 'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 0.8, 'clf__min_samples_split': 50}\n",
    "# without pca, with scalar\n",
    "# 0.884116233304\n",
    "# {'clf__criterion': 'entropy', 'clf__max_depth': 4.1, 'clf__max_features': 0.9, 'clf__min_samples_split': 2}\n",
    "# without pca, without scalar\n",
    "# 0.882875577875\n",
    "# {'clf__criterion': 'entropy', 'clf__max_depth': 4, 'clf__max_features': 0.9, 'clf__min_samples_split': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imputer = Imputer(strategy='median', missing_values=-1)\n",
    "scalar = StandardScaler()\n",
    "pca = PCA()\n",
    "\n",
    "classifier = LinearSVC()\n",
    "\n",
    "params = {\n",
    "    'pca__n_components': np.arange(2,features.shape[1]+1).tolist() ,\n",
    "    #'clf__penalty':['l2','l1'],\n",
    "    #'clf__loss': ['hinge','squared_hinge'],\n",
    "    'clf__class_weight': [None, 'auto'],\n",
    "    'clf__C': [1.0, 10.0, 100.0, 1000.0],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scl', scalar),\n",
    "    ('imp', imputer),\n",
    "    ('pca', pca),\n",
    "    ('clf', classifier),\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, params, cv=10, scoring='roc_auc')\n",
    "grid_search.fit(features.values, target)\n",
    "print grid_search.best_score_\n",
    "print grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predicting Test Values for Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = grid_search2.predict(kaggle_data.drop('PassengerId',1).values)\n",
    "\n",
    "gb_pred = pd.concat([kaggle_data['PassengerId'].reset_index(drop=True), \n",
    "                     pd.DataFrame(data=pred, columns=['Survived'])], axis=1)\n",
    "gb_pred.to_csv('GB_classifier_20150709_v4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(training_features, target)\n",
    "pred = rf.predict(kaggle_data.drop('PassengerId',1).values)\n",
    "\n",
    "rf_pred = pd.concat([kaggle_data['PassengerId'].reset_index(drop=True), \n",
    "                     pd.DataFrame(data=pred, columns=['Survived'])], axis=1)\n",
    "rf_pred.to_csv('Titanic_classifier_20150709_v5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Found random solution online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, Imputer\n",
    "from patsy import dmatrices, dmatrix\n",
    "\n",
    "#Print you can execute arbitrary python code\n",
    "#Split values based on the two CSVs\n",
    "df_train = pd.read_csv(\"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\", \n",
    "                       dtype={\"Age\": np.float64} )\n",
    "df_test = pd.read_csv(\"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\", \n",
    "                    dtype={\"Age\": np.float64} )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Summary statistics of training data\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   712.000000  712.000000  712.000000  712.000000  712.000000   \n",
      "mean    448.589888    0.404494    2.240169   29.642093    0.514045   \n",
      "std     258.683191    0.491139    0.836854   14.492933    0.930692   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     222.750000    0.000000    1.000000   20.000000    0.000000   \n",
      "50%     445.000000    0.000000    2.000000   28.000000    0.000000   \n",
      "75%     677.250000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    5.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  712.000000  712.000000  \n",
      "mean     0.432584   34.567251  \n",
      "std      0.854181   52.938648  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    8.050000  \n",
      "50%      0.000000   15.645850  \n",
      "75%      1.000000   33.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "# Drop NaNs\n",
    "df_train.dropna(subset=['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'], inplace=True)\n",
    "\n",
    "print(\"\\n\\nSummary statistics of training data\")\n",
    "print(df_train.describe())\n",
    "\n",
    "# Age imputation\n",
    "df_train.loc[df_train['Age'].isnull(), 'Age'] = np.nanmedian(df_train['Age'])\n",
    "df_test.loc[df_test['Age'].isnull(), 'Age'] = np.nanmedian(df_test['Age'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training/testing array creation\n",
    "y_train, X_train = dmatrices('Survived ~ Age + Sex + Pclass + SibSp + Parch + Embarked', df_train)\n",
    "X_test = dmatrix('Age + Sex + Pclass + SibSp + Parch + Embarked', df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Logistic Regression-Poly Features (cubic)): 0.8357\n",
      "Accuracy (Random Forest - Calibration): 0.7739\n"
     ]
    }
   ],
   "source": [
    "# Creating processing pipelines with preprocessing. Hyperparameters selected using cross validation\n",
    "steps1 = [('poly_features', PolynomialFeatures(3, interaction_only=True)),\n",
    "          ('logistic', LogisticRegression(C=5555., max_iter=16, penalty='l2'))]\n",
    "steps2 = [('rforest', RandomForestClassifier(min_samples_split=15, n_estimators=73, criterion='entropy'))]\n",
    "pipeline1 = Pipeline(steps=steps1)\n",
    "pipeline2 = Pipeline(steps=steps2)\n",
    "\n",
    "# Logistic model with cubic features\n",
    "pipeline1.fit(X_train, y_train.ravel())\n",
    "print('Accuracy (Logistic Regression-Poly Features (cubic)): {:.4f}'.format(pipeline1.score(X_train, y_train.ravel())))\n",
    "\n",
    "# Random forest with calibration\n",
    "pipeline2.fit(X_train[:600], y_train[:600].ravel())\n",
    "calibratedpipe2 = CalibratedClassifierCV(pipeline2, cv=3, method='sigmoid')\n",
    "calibratedpipe2.fit(X_train[600:], y_train[600:].ravel())\n",
    "print('Accuracy (Random Forest - Calibration): {:.4f}'.format(calibratedpipe2.score(X_train, y_train.ravel())))\n",
    "\n",
    "# Create the output dataframe\n",
    "output = pd.DataFrame(columns=['PassengerId', 'Survived'])\n",
    "output['PassengerId'] = df_test['PassengerId']\n",
    "\n",
    "# Predict the survivors and output csv\n",
    "output['Survived'] = pipeline1.predict(X_test).astype(int)\n",
    "output.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop NaNs\n",
    "df_train.dropna(subset=['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'], inplace=True)\n",
    "\n",
    "print(\"\\n\\nSummary statistics of training data\")\n",
    "print(df_train.describe())\n",
    "\n",
    "# Age imputation\n",
    "df_train.loc[df_train['Age'].isnull(), 'Age'] = np.nanmedian(df_train['Age'])\n",
    "df_test.loc[df_test['Age'].isnull(), 'Age'] = np.nanmedian(df_test['Age'])\n",
    "\n",
    "# Training/testing array creation\n",
    "y_train, X_train = dmatrices('Survived ~ Age + Sex + Pclass + SibSp + Parch + Embarked', df_train)\n",
    "X_test = dmatrix('Age + Sex + Pclass + SibSp + Parch + Embarked', df_test)\n",
    "\n",
    "# Creating processing pipelines with preprocessing. Hyperparameters selected using cross validation\n",
    "steps1 = [('poly_features', PolynomialFeatures(3, interaction_only=True)),\n",
    "          ('logistic', LogisticRegression(C=5555., max_iter=16, penalty='l2'))]\n",
    "steps2 = [('rforest', RandomForestClassifier(min_samples_split=15, n_estimators=73, criterion='entropy'))]\n",
    "pipeline1 = Pipeline(steps=steps1)\n",
    "pipeline2 = Pipeline(steps=steps2)\n",
    "\n",
    "# Logistic model with cubic features\n",
    "pipeline1.fit(X_train, y_train.ravel())\n",
    "print('Accuracy (Logistic Regression-Poly Features (cubic)): {:.4f}'.format(pipeline1.score(X_train, y_train.ravel())))\n",
    "\n",
    "# Random forest with calibration\n",
    "pipeline2.fit(X_train[:600], y_train[:600].ravel())\n",
    "calibratedpipe2 = CalibratedClassifierCV(pipeline2, cv=3, method='sigmoid')\n",
    "calibratedpipe2.fit(X_train[600:], y_train[600:].ravel())\n",
    "print('Accuracy (Random Forest - Calibration): {:.4f}'.format(calibratedpipe2.score(X_train, y_train.ravel())))\n",
    "\n",
    "# Create the output dataframe\n",
    "output = pd.DataFrame(columns=['PassengerId', 'Survived'])\n",
    "output['PassengerId'] = df_test['PassengerId']\n",
    "\n",
    "# Predict the survivors and output csv\n",
    "output['Survived'] = pipeline1.predict(X_test).astype(int)\n",
    "output.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
