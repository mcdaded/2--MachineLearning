{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#Split values based on the two CSVs\n",
    "training = pd.read_csv(\"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\")\n",
    "testing = pd.read_csv(\"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\")\n",
    "\n",
    "data = training\n",
    "target = data['Survived'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"6\" halign=\"left\">PassengerId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <th colspan=\"3\" halign=\"left\">female</th>\n",
       "      <th colspan=\"3\" halign=\"left\">male</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.123288</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.694737</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.825397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.876712</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.174603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PassengerId                                              \n",
       "Sex           female                      male                    \n",
       "Embarked           C     Q         S         C         Q         S\n",
       "Survived                                                          \n",
       "0           0.123288  0.25  0.310345  0.694737  0.926829  0.825397\n",
       "1           0.876712  0.75  0.689655  0.305263  0.073171  0.174603"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show percentage survivors by sex and by class\n",
    "pvt_train = pd.pivot_table(data=training, index=['Survived'], columns=['Sex', 'Embarked'], values =['PassengerId'], aggfunc='count')\n",
    "pvt_train.apply(lambda x: x / (x.max() + x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will try to loop through the different classes and sex to apply the median age\n",
    "for sex in xrange(0,2):\n",
    "    for cabin_class in xrange(0,4):\n",
    "        median_age_group = features[(features.Pclass == cabin_class) & (features.Sex_female == sex)].dropna().median()\n",
    "        features[(features.Pclass == cabin_class) \n",
    "                 & (features.Sex_female == sex)] = features[(features.Pclass == cabin_class) \n",
    "                                                                     & (features.Sex_female == sex)\n",
    "                                                                    ].fillna(median_age_group)\n",
    "# adding validation for if the person is a child\n",
    "features['child'] = (features.Age <= 18)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799451201536\n",
      "{'alpha': 0.005}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "mnb = BernoulliNB()\n",
    "\n",
    "params = {\n",
    "    'alpha': [0.005, 0.0051, 0.00059],\n",
    "}\n",
    "mnbgs = GridSearchCV(mnb, params, cv=5, scoring='roc_auc')\n",
    "mnbgs.fit(features, target)\n",
    "\n",
    "print mnbgs.best_score_\n",
    "print mnbgs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.868623852281\n",
      "{'max_features': 0.9, 'max_depth': 4}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "params = {\n",
    "    'max_features': [0.75, 0.8, 0.9],\n",
    "    'max_depth': [3.75, 3.9, 4, 4.1],\n",
    "}\n",
    "rgs = GridSearchCV(rf, params, cv=5, scoring='roc_auc')\n",
    "rgs.fit(features, target)\n",
    "print rgs.best_score_\n",
    "print rgs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.866916607304\n",
      "{'max_features': 0.75, 'max_depth': 4.1}\n"
     ]
    }
   ],
   "source": [
    "print rgs.best_score_\n",
    "print rgs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.877983240881\n",
      "{'max_features': 0.8, 'learning_rate': 0.2, 'max_depth': 3.75}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=100, subsample=.8)\n",
    "\n",
    "params = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2, 0.5],\n",
    "    'max_features': [0.7, 0.75, 0.8],\n",
    "    'max_depth': [ 3.5, 3.6, 3.75, 3.9, 4],\n",
    "}\n",
    "gs = GridSearchCV(gb, params, cv=10, scoring='roc_auc')\n",
    "gs.fit(features, target)\n",
    "print gs.best_score_\n",
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.878693523426\n",
      "{'max_features': 0.75, 'learning_rate': 0.1, 'max_depth': 3.5}\n"
     ]
    }
   ],
   "source": [
    "print gs.best_score_\n",
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# [['Pclass','SibSp','Parch','child','Survived','sex_bool']]\n",
    "data = pd.read_csv(\"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\")\n",
    "data['embarked_val'] = data['Embarked'].dropna().map({'S': 1, 'C': 2, 'Q': 3}).astype(int)\n",
    "data['child'] = (data.Age.dropna() <= 18)*1\n",
    "data['sex_bool'] = (data.Sex.dropna() == 'male')*1\n",
    "cleaned_data = data[['Pclass','child','Survived','sex_bool','embarked_val']].dropna() \n",
    "X = cleaned_data[['Pclass','child','sex_bool','embarked_val']]\n",
    "y = cleaned_data[['Survived']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#-------------------------------------------------------------------\n",
    "# pipelines and scalars\n",
    "#-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.decomposition import PCA, RandomizedPCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex  Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male   22      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   38      1   \n",
       "2                             Heikkinen, Miss. Laina  female   26      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female   35      1   \n",
       "4                           Allen, Mr. William Henry    male   35      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"9\" halign=\"left\">PassengerId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family_size</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.696462</td>\n",
       "      <td>0.447205</td>\n",
       "      <td>0.421569</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.303538</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            PassengerId                                               \\\n",
       "Family_size          0         1         2         3    4         5    \n",
       "Survived                                                               \n",
       "0              0.696462  0.447205  0.421569  0.275862  0.8  0.863636   \n",
       "1              0.303538  0.552795  0.578431  0.724138  0.2  0.136364   \n",
       "\n",
       "                                 \n",
       "Family_size        6    7    10  \n",
       "Survived                         \n",
       "0            0.666667  0.5  0.5  \n",
       "1            0.333333  NaN  NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data['expensive_coach_woman'] = ((data.Sex == 'female') & (data.Pclass==3) & (data.Fare >= 20)) * 1\n",
    "pvt_train = pd.pivot_table(data=data, index=['Survived'], columns=['Family_size'], values =['PassengerId'], aggfunc='count')\n",
    "pvt_train.apply(lambda x: x / (x.max() + x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fare</th>\n",
       "      <th>Age</th>\n",
       "      <th>expensive_coach_woman</th>\n",
       "      <th>Family_size</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Dr</th>\n",
       "      <th>Lady</th>\n",
       "      <th>Master</th>\n",
       "      <th>Miss</th>\n",
       "      <th>Mr</th>\n",
       "      <th>Mrs</th>\n",
       "      <th>Rev</th>\n",
       "      <th>Sir</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.2500</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71.2833</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.9250</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.1000</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0500</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Fare  Age  expensive_coach_woman  Family_size  Sex_female  Pclass_1  \\\n",
       "0   7.2500   22                      0            2           0         0   \n",
       "1  71.2833   38                      0            2           1         1   \n",
       "2   7.9250   26                      0            1           1         0   \n",
       "3  53.1000   35                      0            2           1         1   \n",
       "4   8.0500   35                      0            1           0         0   \n",
       "\n",
       "   Pclass_2  Pclass_3  Dr  Lady  Master  Miss  Mr  Mrs  Rev  Sir  Embarked_C  \\\n",
       "0         0         1   0     0       0     0   1    0    0    0           0   \n",
       "1         0         0   0     0       0     0   0    1    0    0           1   \n",
       "2         0         1   0     0       0     1   0    0    0    0           0   \n",
       "3         0         0   0     0       0     0   0    1    0    0           0   \n",
       "4         0         1   0     0       0     0   1    0    0    0           0   \n",
       "\n",
       "   Embarked_Q  Embarked_S  \n",
       "0           0           1  \n",
       "1           0           0  \n",
       "2           0           1  \n",
       "3           0           1  \n",
       "4           0           1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data['child'] = (data.Age.dropna() <= 18)*1\n",
    "#feature engineering\n",
    "data['expensive_coach_woman'] = ((data.Sex == 'female') & (data.Pclass==3) & (data.Fare >= 20)) * 1\n",
    "# based on looking at the values there are a few that appear very infrequently, we will map them to more useful values\n",
    "title_map = {'Capt': 'Sir', 'Don':'Sir', 'Major':'Sir', 'Sir':'Sir', 'Col':'Sir', 'Mlle':'Sir', 'Jonkheer':'Sir',\n",
    "             'Mme': 'Lady', 'Lady':'Lady', 'the Countess':'Lady', 'Ms': 'Miss'}\n",
    "def title_extract(name):\n",
    "    title = name.split(',')[1].split('.')[0].strip()\n",
    "    if title in title_map:\n",
    "        title = title_map[title]\n",
    "    return title\n",
    "data['Title'] = data.Name.apply(lambda x: title_extract(x))\n",
    "\n",
    "data['Surname'] = data.Name.apply(lambda x: x.split(',')[0])\n",
    "data['Family_size'] = data.SibSp + data.Parch + 1\n",
    "data['FamilyID'] = (data.Family_size.astype(str) + data.Surname) * ((data.Family_size >= 2) * 1)\n",
    "data['FamilyID'] = data['FamilyID'].apply(lambda x: 'small' if x == '' else x)\n",
    "\n",
    "features = pd.concat([data[['Fare', 'Age', 'expensive_coach_woman', 'Family_size']],\n",
    "                      pd.get_dummies(data['Sex'], prefix='Sex'),\n",
    "                      pd.get_dummies(data['Pclass'], prefix='Pclass'),\n",
    "                      pd.get_dummies(data['Title']),\n",
    "                      pd.get_dummies(data['Embarked'], prefix='Embarked'), \n",
    "                     ],\n",
    "                     axis=1)\n",
    "features = features.drop('Sex_male', 1)\n",
    "\n",
    "# Because of the following bug we cannot use NaN as the missing\n",
    "# value marker, use a negative value as marker instead:\n",
    "# https://github.com/scikit-learn/scikit-learn/issues/3044\n",
    "features = features.fillna(-1)\n",
    "features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedPCA(copy=True, iterated_power=3, n_components=None,\n",
       "       random_state=None, whiten=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = RandomizedPCA()\n",
    "pca.fit(pd.concat([pd.DataFrame(target), features], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8.86529590e-01   1.11927831e-01   8.81734402e-04   2.20437110e-04\n",
      "   1.14540870e-04   9.51323119e-05   5.90607733e-05   5.38733937e-05\n",
      "   4.03863872e-05   3.36858247e-05   2.18496721e-05   8.90949415e-06\n",
      "   4.94619979e-06   3.25734262e-06   2.50446361e-06   1.65305541e-06\n",
      "   3.45522667e-07   2.62309355e-07   1.19070628e-36   2.25383453e-37]\n"
     ]
    }
   ],
   "source": [
    "print pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  components describe  .99 % of the variance\n"
     ]
    }
   ],
   "source": [
    "# Minimum percentage of variance we want to be described by the resulting transformed components\n",
    "variance_pct = .99\n",
    " \n",
    "# Create PCA object\n",
    "pca = PCA(n_components=variance_pct)\n",
    " \n",
    "# Transform the initial features\n",
    "X_transformed = pca.fit_transform(features.values, target)\n",
    " \n",
    "# Create a data frame from the PCA'd data\n",
    "pcaDataFrame = pd.DataFrame(X_transformed)\n",
    " \n",
    "print pcaDataFrame.shape[1], \" components describe \", str(variance_pct)[1:], \"% of the variance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "('min -', 0.5977653631284916, 'mean -', 0.67129747743357926, 'max -', 0.7078651685393258)\n",
      "\n",
      "\n",
      "\n",
      "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
      "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "('min -', 0.62011173184357538, 'mean -', 0.69923001276337537, 'max -', 0.7415730337078652)\n",
      "\n",
      "\n",
      "\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "('min -', 0.58659217877094971, 'mean -', 0.66458764438590567, 'max -', 0.72881355932203384)\n",
      "\n",
      "\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0)\n",
      "('min -', 0.5977653631284916, 'mean -', 0.66333779820958116, 'max -', 0.7039106145251397)\n",
      "\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            random_state=None, splitter='best')\n",
      "('min -', 0.59217877094972071, 'mean -', 0.67570976105642022, 'max -', 0.7247191011235955)\n",
      "\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_neighbors=5, p=2, weights='uniform')\n",
      "('min -', 0.60893854748603349, 'mean -', 0.64205860229868428, 'max -', 0.67796610169491522)\n",
      "\n",
      "\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "('min -', 0.56424581005586594, 'mean -', 0.62864414936305468, 'max -', 0.6853932584269663)\n",
      "\n",
      "\n",
      "\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "('min -', 0.4887640449438202, 'mean -', 0.60287727406360836, 'max -', 0.7078651685393258)\n",
      "\n",
      "\n",
      "\n",
      "GaussianNB()\n",
      "('min -', 0.5977653631284916, 'mean -', 0.6678197679470117, 'max -', 0.72625698324022347)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer = Imputer(strategy='median', missing_values=-1)\n",
    "scalar = StandardScaler()\n",
    "pca = PCA()\n",
    "classifiers = [RandomForestClassifier(), GradientBoostingClassifier(), \n",
    "               ExtraTreesClassifier(), LogisticRegression(), DecisionTreeClassifier(),\n",
    "               KNeighborsClassifier(), SVC(), LinearSVC(), GaussianNB()]\n",
    "\n",
    "print '\\n'\n",
    "for classifier in classifiers:\n",
    "    pipeline = Pipeline([\n",
    "        #('scl', scalar),\n",
    "        ('imp', imputer),\n",
    "        ('clf', classifier),\n",
    "    ])\n",
    "    #scores = cross_val_score(pipeline, features.values, target, cv=5, scoring='accuracy')\n",
    "    scores = cross_val_score(pipeline, pcaDataFrame, target, cv=5, scoring='accuracy')\n",
    "    print str(classifier)\n",
    "    print('min -', scores.min(), 'mean -', scores.mean(), 'max -', scores.max())\n",
    "    print '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.873464313\n",
      "{'clf__max_features': 0.78, 'clf__max_depth': 3.6, 'clf__learning_rate': 0.1, 'pca__n_components': 8}\n"
     ]
    }
   ],
   "source": [
    "imputer = Imputer(strategy='median', missing_values=-1)\n",
    "scalar = StandardScaler()\n",
    "pca = PCA()\n",
    "\n",
    "classifier = GradientBoostingClassifier(n_estimators=100, subsample=.8)\n",
    "\n",
    "params = {\n",
    "    'pca__n_components': np.arange(2,features.shape[1]+1).tolist() ,\n",
    "    'clf__learning_rate': [0.1, 0.15, 0.2],\n",
    "    'clf__max_features': [0.76, 0.77, 0.78],\n",
    "    'clf__max_depth': [3.6, 3.63],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('scl', scalar),\n",
    "    ('imp', imputer),\n",
    "    ('pca', pca),\n",
    "    ('clf', classifier),\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, params, cv=10, scoring='roc_auc')\n",
    "grid_search.fit(features.values, target)\n",
    "print grid_search.best_score_\n",
    "print grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imputer = Imputer(strategy='median', missing_values=-1)\n",
    "scalar = StandardScaler()\n",
    "pca = PCA()\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "params = {\n",
    "    'pca__n_components': np.arange(2,features.shape[1]+1).tolist() ,\n",
    "    'clf__criterion':['gini','entropy'],\n",
    "    'clf__max_features': [0.75, 0.8, 0.9],\n",
    "    'clf__max_depth': [3.75, 3.9, 4, 4.1],\n",
    "    'clf__min_samples_split': [2, 10, 50],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scl', scalar),\n",
    "    ('imp', imputer),\n",
    "    ('pca', pca),\n",
    "    ('clf', classifier),\n",
    "])\n",
    "\n",
    "grid_search2 = GridSearchCV(pipeline, params, cv=10, scoring='roc_auc')\n",
    "grid_search2.fit(features.values, target)\n",
    "print grid_search2.best_score_\n",
    "print grid_search2.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: small",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d498c33c0089>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'roc_auc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\sklearn\\grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \"\"\"\n\u001b[1;32m--> 732\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\sklearn\\grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    503\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                                     error_score=self.error_score)\n\u001b[1;32m--> 505\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m                 for train, test in cv)\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \"\"\"\n\u001b[0;32m    405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[0;32m   1457\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1458\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1459\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1461\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \"\"\"\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\sklearn\\pipeline.pyc\u001b[0m in \u001b[0;36m_pre_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\sklearn\\base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\imputation.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             X = check_array(X, accept_sparse='csc', dtype=np.float64,\n\u001b[1;32m--> 154\u001b[1;33m                             force_all_finite=False)\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dan\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features)\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m                 \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m         \u001b[1;31m# make sure we actually converted to numeric:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"O\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: small"
     ]
    }
   ],
   "source": [
    "imputer = Imputer(strategy='median', missing_values=-1)\n",
    "scalar = StandardScaler()\n",
    "pca = PCA()\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100)\n",
    "min_samples_split=15, n_estimators=73, criterion='entropy'\n",
    "params = {\n",
    "    'pca__n_components': np.arange(2,features.shape[1]+1).tolist() ,\n",
    "    'clf__criterion':['gini','entropy'],\n",
    "    'clf__max_features': [0.75, 0.8, 0.9],\n",
    "    'clf__max_depth': [3.75, 3.9, 4, 4.1],\n",
    "    'clf__min_samples_split': [2, 10, 50],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "   #('scl', scalar),\n",
    "    ('imp', imputer),\n",
    "    ('pca', pca),\n",
    "    ('clf', classifier),\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, params, cv=10, scoring='roc_auc')\n",
    "grid_search.fit(features.values, target)\n",
    "print grid_search.best_score_\n",
    "print grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.867379761709\n",
      "{'pca__n_components': 15, 'clf__C': 1.0, 'clf__class_weight': None}\n"
     ]
    }
   ],
   "source": [
    "imputer = Imputer(strategy='median', missing_values=-1)\n",
    "scalar = StandardScaler()\n",
    "pca = PCA()\n",
    "\n",
    "classifier = LinearSVC()\n",
    "\n",
    "params = {\n",
    "    'pca__n_components': np.arange(2,features.shape[1]+1).tolist() ,\n",
    "    #'clf__penalty':['l2','l1'],\n",
    "    #'clf__loss': ['hinge','squared_hinge'],\n",
    "    'clf__class_weight': [None, 'auto'],\n",
    "    'clf__C': [1.0, 10.0, 100.0, 1000.0],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scl', scalar),\n",
    "    ('imp', imputer),\n",
    "    ('pca', pca),\n",
    "    ('clf', classifier),\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, params, cv=10, scoring='roc_auc')\n",
    "grid_search.fit(features.values, target)\n",
    "print grid_search.best_score_\n",
    "print grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predicting Test Values for Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fare</th>\n",
       "      <th>Age</th>\n",
       "      <th>expensive_coach_woman</th>\n",
       "      <th>Family_size</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Dona</th>\n",
       "      <th>Dr</th>\n",
       "      <th>Master</th>\n",
       "      <th>Miss</th>\n",
       "      <th>Mr</th>\n",
       "      <th>Mrs</th>\n",
       "      <th>Rev</th>\n",
       "      <th>Sir</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.8292</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0000</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.6875</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.6625</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.2875</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Fare   Age  expensive_coach_woman  Family_size  Sex_female  Pclass_1  \\\n",
       "0   7.8292  34.5                      0            1           0         0   \n",
       "1   7.0000  47.0                      0            2           1         0   \n",
       "2   9.6875  62.0                      0            1           0         0   \n",
       "3   8.6625  27.0                      0            1           0         0   \n",
       "4  12.2875  22.0                      0            3           1         0   \n",
       "\n",
       "   Pclass_2  Pclass_3  Dona  Dr  Master  Miss  Mr  Mrs  Rev  Sir  Embarked_C  \\\n",
       "0         0         1     0   0       0     0   1    0    0    0           0   \n",
       "1         0         1     0   0       0     0   0    1    0    0           0   \n",
       "2         1         0     0   0       0     0   1    0    0    0           0   \n",
       "3         0         1     0   0       0     0   1    0    0    0           0   \n",
       "4         0         1     0   0       0     0   0    1    0    0           0   \n",
       "\n",
       "   Embarked_Q  Embarked_S  \n",
       "0           1           0  \n",
       "1           0           1  \n",
       "2           1           0  \n",
       "3           0           1  \n",
       "4           0           1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing['expensive_coach_woman'] = ((testing.Sex == 'female') & (testing.Pclass==3) & (testing.Fare >= 20)) * 1\n",
    "# based on looking at the values there are a few that appear very infrequently, we will map them to more useful values\n",
    "title_map = {'Capt': 'Sir', 'Don':'Sir', 'Major':'Sir', 'Sir':'Sir', 'Col':'Sir', 'Mlle':'Sir', 'Jonkheer':'Sir',\n",
    "             'Mme': 'Lady', 'Lady':'Lady', 'the Countess':'Lady', 'Ms': 'Miss'}\n",
    "def title_extract(name):\n",
    "    title = name.split(',')[1].split('.')[0].strip()\n",
    "    if title in title_map:\n",
    "        title = title_map[title]\n",
    "    return title\n",
    "testing['Title'] = testing.Name.apply(lambda x: title_extract(x))\n",
    "\n",
    "testing['Surname'] = testing.Name.apply(lambda x: x.split(',')[0])\n",
    "testing['Family_size'] = testing.SibSp + testing.Parch + 1\n",
    "testing['FamilyID'] = (testing.Family_size.astype(str) + testing.Surname) * ((testing.Family_size >= 2) * 1)\n",
    "testing['FamilyID'] = testing['FamilyID'].apply(lambda x: 'small' if x == '' else x)\n",
    "\n",
    "testing_features = pd.concat([testing[['Fare', 'Age', 'expensive_coach_woman', 'Family_size']],\n",
    "                      pd.get_dummies(testing['Sex'], prefix='Sex'),\n",
    "                      pd.get_dummies(testing['Pclass'], prefix='Pclass'),\n",
    "                      pd.get_dummies(testing['Title']),\n",
    "                      pd.get_dummies(testing['Embarked'], prefix='Embarked')\n",
    "                     ],\n",
    "                     axis=1)\n",
    "testing_features = testing_features.drop('Sex_male', 1)\n",
    "\n",
    "# Because of the following bug we cannot use NaN as the missing\n",
    "# value marker, use a negative value as marker instead:\n",
    "# https://github.com/scikit-learn/scikit-learn/issues/3044\n",
    "testing_features = testing_features.fillna(-1)\n",
    "testing_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = grid_search.predict(testing_features)\n",
    "\n",
    "gb_pred = pd.concat([testing['PassengerId'], pd.DataFrame(data=pred, columns=['Survived'])], axis=1)\n",
    "gb_pred.to_csv('GB_classifier_20150705.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, Imputer\n",
    "from patsy import dmatrices, dmatrix\n",
    "\n",
    "#Print you can execute arbitrary python code\n",
    "#Split values based on the two CSVs\n",
    "df_train = pd.read_csv(\"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\", \n",
    "                       dtype={\"Age\": np.float64} )\n",
    "df_test = pd.read_csv(\"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\", \n",
    "                    dtype={\"Age\": np.float64} )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Summary statistics of training data\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   712.000000  712.000000  712.000000  712.000000  712.000000   \n",
      "mean    448.589888    0.404494    2.240169   29.642093    0.514045   \n",
      "std     258.683191    0.491139    0.836854   14.492933    0.930692   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     222.750000    0.000000    1.000000   20.000000    0.000000   \n",
      "50%     445.000000    0.000000    2.000000   28.000000    0.000000   \n",
      "75%     677.250000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    5.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  712.000000  712.000000  \n",
      "mean     0.432584   34.567251  \n",
      "std      0.854181   52.938648  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    8.050000  \n",
      "50%      0.000000   15.645850  \n",
      "75%      1.000000   33.000000  \n",
      "max      6.000000  512.329200  \n",
      "Accuracy (Logistic Regression-Poly Features (cubic)): 0.8413\n",
      "Accuracy (Random Forest - Calibration): 0.7865\n"
     ]
    }
   ],
   "source": [
    "# Drop NaNs\n",
    "df_train.dropna(subset=['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'], inplace=True)\n",
    "\n",
    "print(\"\\n\\nSummary statistics of training data\")\n",
    "print(df_train.describe())\n",
    "\n",
    "# Age imputation\n",
    "df_train.loc[df_train['Age'].isnull(), 'Age'] = np.nanmedian(df_train['Age'])\n",
    "df_test.loc[df_test['Age'].isnull(), 'Age'] = np.nanmedian(df_test['Age'])\n",
    "\n",
    "# Training/testing array creation\n",
    "y_train, X_train = dmatrices('Survived ~ Age + Sex + Pclass + SibSp + Parch + Embarked', df_train)\n",
    "X_test = dmatrix('Age + Sex + Pclass + SibSp + Parch + Embarked', df_test)\n",
    "\n",
    "# Creating processing pipelines with preprocessing. Hyperparameters selected using cross validation\n",
    "steps1 = [('poly_features', PolynomialFeatures(3, interaction_only=True)),\n",
    "          ('logistic', LogisticRegression(C=5555., max_iter=16, penalty='l2'))]\n",
    "steps2 = [('rforest', RandomForestClassifier(min_samples_split=15, n_estimators=73, criterion='entropy'))]\n",
    "pipeline1 = Pipeline(steps=steps1)\n",
    "pipeline2 = Pipeline(steps=steps2)\n",
    "\n",
    "# Logistic model with cubic features\n",
    "pipeline1.fit(X_train, y_train.ravel())\n",
    "print('Accuracy (Logistic Regression-Poly Features (cubic)): {:.4f}'.format(pipeline1.score(X_train, y_train.ravel())))\n",
    "\n",
    "# Random forest with calibration\n",
    "pipeline2.fit(X_train[:600], y_train[:600].ravel())\n",
    "calibratedpipe2 = CalibratedClassifierCV(pipeline2, cv=3, method='sigmoid')\n",
    "calibratedpipe2.fit(X_train[600:], y_train[600:].ravel())\n",
    "print('Accuracy (Random Forest - Calibration): {:.4f}'.format(calibratedpipe2.score(X_train, y_train.ravel())))\n",
    "\n",
    "# Create the output dataframe\n",
    "output = pd.DataFrame(columns=['PassengerId', 'Survived'])\n",
    "output['PassengerId'] = df_test['PassengerId']\n",
    "\n",
    "# Predict the survivors and output csv\n",
    "output['Survived'] = pipeline1.predict(X_test).astype(int)\n",
    "output.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop NaNs\n",
    "df_train.dropna(subset=['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'], inplace=True)\n",
    "\n",
    "print(\"\\n\\nSummary statistics of training data\")\n",
    "print(df_train.describe())\n",
    "\n",
    "# Age imputation\n",
    "df_train.loc[df_train['Age'].isnull(), 'Age'] = np.nanmedian(df_train['Age'])\n",
    "df_test.loc[df_test['Age'].isnull(), 'Age'] = np.nanmedian(df_test['Age'])\n",
    "\n",
    "# Training/testing array creation\n",
    "y_train, X_train = dmatrices('Survived ~ Age + Sex + Pclass + SibSp + Parch + Embarked', df_train)\n",
    "X_test = dmatrix('Age + Sex + Pclass + SibSp + Parch + Embarked', df_test)\n",
    "\n",
    "# Creating processing pipelines with preprocessing. Hyperparameters selected using cross validation\n",
    "steps1 = [('poly_features', PolynomialFeatures(3, interaction_only=True)),\n",
    "          ('logistic', LogisticRegression(C=5555., max_iter=16, penalty='l2'))]\n",
    "steps2 = [('rforest', RandomForestClassifier(min_samples_split=15, n_estimators=73, criterion='entropy'))]\n",
    "pipeline1 = Pipeline(steps=steps1)\n",
    "pipeline2 = Pipeline(steps=steps2)\n",
    "\n",
    "# Logistic model with cubic features\n",
    "pipeline1.fit(X_train, y_train.ravel())\n",
    "print('Accuracy (Logistic Regression-Poly Features (cubic)): {:.4f}'.format(pipeline1.score(X_train, y_train.ravel())))\n",
    "\n",
    "# Random forest with calibration\n",
    "pipeline2.fit(X_train[:600], y_train[:600].ravel())\n",
    "calibratedpipe2 = CalibratedClassifierCV(pipeline2, cv=3, method='sigmoid')\n",
    "calibratedpipe2.fit(X_train[600:], y_train[600:].ravel())\n",
    "print('Accuracy (Random Forest - Calibration): {:.4f}'.format(calibratedpipe2.score(X_train, y_train.ravel())))\n",
    "\n",
    "# Create the output dataframe\n",
    "output = pd.DataFrame(columns=['PassengerId', 'Survived'])\n",
    "output['PassengerId'] = df_test['PassengerId']\n",
    "\n",
    "# Predict the survivors and output csv\n",
    "output['Survived'] = pipeline1.predict(X_test).astype(int)\n",
    "output.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
